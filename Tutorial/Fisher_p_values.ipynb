{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8f6e0b",
   "metadata": {},
   "source": [
    "# Tutorial Assignment\n",
    "\n",
    "- Your mark will be based on the TA checking autotests and then manually reviewing your answers to `Q1` and `Q8`\n",
    "- Your answers can be reviewed against the example answers avaialble through MarkUs\n",
    "\n",
    "## Fisher's Tea Experiment\n",
    "\n",
    "A most beloved piece of [statistical lore](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2012.00620.x) about about the (most famous) statistician Ronald Fisher involves cups of tea with milk. Fisher and his friend and colleague, Dr. Muriel Bristol, worked at Cambridge in the 1920s and regularly had tea together. During one of their afternoon tea times, Bristol refused a cup of tea from Fisher because he put milk in first BEFORE pouring in the tea. Bristol said she could taste the difference, and much preferred the taste of tea when the milk was poured in afterward the tea. Fisher didn't think that there could be a difference and proposed a hypothesis test to examine the situation.\n",
    "\n",
    "Fisher made 8 cups of tea, 4 with milk added in first and 4 with tea added in first, and gave them to Dr. Bristol without her seeing how they were made and she would say if she thought the tea or the milk was poured first.  As it turned out, Bristol correctly identified if the tea or milk was poured first for all 8 of the cups. Fisher, being a skeptical statistician wanted to test if this could be happening by chance with Bristol just randomly guessing (or whether there was evidence against an assumption of Bristol just randomly guessing).\n",
    "\n",
    "Suppose you run an experiment like this with students in STA130. You get a random sample of 80 STA130 students to each taste one cup of tea and tell you whether they think the milk or tea was poured first. Suppose 49 students are able to correctly state which was poured first. Provide a statistical analysis of this experiment as guided through the following set of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffdb1c",
   "metadata": {},
   "source": [
    "### Q0: Indicate what the population, parameter $p$, sample, and test statistic $\\hat p$ under consideration here are; and, indicate what the observed value of the test statistic is.\n",
    "\n",
    "> - Hint 1: putting a \"hat\" on a parameter symbol, such as $\\hat p$ is the preferred statistical notation to refer to test statistic that could estimate a parameter; and, assigning a value such as $\\hat p=0.33$ indicates the observed value of the test statistic for the sample in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e4616",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n",
    "- Population:\n",
    "- Parameter $p$:\n",
    "- Sample:\n",
    "- Test Statistic $\\hat p$:\n",
    "- Observed Test Statistic $\\hat p = \\cdots$: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028812b3",
   "metadata": {},
   "source": [
    "### Q1: Ignoring the question of sample size, what is the difference between the experiment with STA130 students and the original context of the experiment with Fisher and Bristol?\n",
    "\n",
    "> - Hint 1: the parameter is more personalized in the original experiment; whereas, the parameter in the context of STA130 students is a more abstract\n",
    "> - Hint 2: this could be discussed from the perspective of differences in what the population is; or, what the sample is comprised of; or, what the parameter means, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bedd53",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc62b3d",
   "metadata": {},
   "source": [
    "### Q2: For the experiment with STA130 students, state a formal *null hypotheses* $H_0$ in terms of parameter $p$, give a written statement specifying the claim of the *null hypothesis* in informal casual everyday common language, and provide an *alternative hypothesis* $H_1$ in terms of $H_0$.\n",
    "\n",
    "> - Hint: Since the population, parameter, sample, test statistic, and observed test statistic are clear from the previous questions, don't worry about specifying those explicitly again here; however, it's not bad practice to include these details in conjunction with a *null hypothesis* statements so the experiment and context are always clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdb95c",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301beee7",
   "metadata": {},
   "source": [
    "### Q3: Examine the analysis below and describe what each single dot in the plot represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654feae1",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "np.random.seed(200)\n",
    "reps = 100\n",
    "num_observations = 80\n",
    "observed_test_stat = 49/num_observations\n",
    "print(observed_test_stat)\n",
    "sim_results = []\n",
    "\n",
    "\n",
    "for i in range(reps):\n",
    "    sim = np.random.choice([\"Milk first\", \"Tea first\"], size=num_observations)\n",
    "    sim_results += [np.count_nonzero(sim == \"Milk first\") / num_observations]\n",
    "    \n",
    "# Code below to output a dotplot\n",
    "encountered_so_far = {}\n",
    "sim_results_height = []\n",
    "for entry in sim_results:\n",
    "    if entry not in encountered_so_far:\n",
    "        encountered_so_far[entry] = 1\n",
    "    else:\n",
    "        encountered_so_far[entry] += 1\n",
    "    sim_results_height.append(encountered_so_far[entry])\n",
    "\n",
    "dataframe = pd.DataFrame({'Proportion Milk First': sim_results, 'Number': sim_results_height, 'size': 1})\n",
    "fig = px.scatter(dataframe, x='Proportion Milk First', y='Number', size='size', size_max=15)\n",
    "fig.add_vline(x=observed_test_stat, line_dash=\"dash\", line_color=\"black\",\n",
    "              annotation_text=\"observed_test_stat (0.6125)\")\n",
    "fig.add_vline(x=1-observed_test_stat, line_dash=\"dash\", line_color=\"black\",\n",
    "              annotation_text=\"(1-0.6125)\")\n",
    "fig.update_layout(xaxis_title=\"Proportion choosing milk first assuming no ability to distinguish (i.e., just guessing)\")\n",
    "fig.update_yaxes(visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63038cc7",
   "metadata": {},
   "source": [
    "### Q4: Describe what the distribution given by all the dots in the plot above represents.\n",
    "\n",
    "> - Hint: you should be discussing the \"simulation\" of a \"sampling distribution\" for a specific \"test statistic\" under the assumptions of a *null hypothesis*..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2b44d",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbaa7c8",
   "metadata": {},
   "source": [
    "### Q5: Report the simulated \"two-sided\" p-value found in the analysis above\n",
    "\n",
    "> - Hint 1: A \"two-sided\" p-value includes simulated test statistics that are both larger than and smaller than the (null) hypothesized parameter value that are \"as or more extreme\" than the observed test statistic (relative to the parameter value in the *null hypothesis*) \n",
    "(and more extreme )\n",
    "> - Hint 2: These simulated p-values are not necessarily particularly precise since they are based on a quite small (and hence a quite roughly approximated) simulation of the samplilng distribution; nonetheless, treat the observed p-value resolution as sufficiently refined and accurate for the purposes of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5 = None # Your answer should be a ratio of integers\n",
    "# E.g., 1/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503f1bd",
   "metadata": {},
   "source": [
    "### Q6: Report the simulated \"one-sided\" p-value found in the analysis above\n",
    "\n",
    "> - Hint 1: A \"one-sided\" p-value only includes simulated tests statistics that are \"as or more extreme\" than the observed test statistic in the same direction as the observed test statistic (relative to the parameter value in the *null hypothesis*) \n",
    "> - Hint 2: These simulated p-values are not necessarily particularly precise since they are based on a quite small (and hence a quite roughly approximated) simulation of the samplilng distribution; nonetheless, treat the observed p-value resolution as sufficiently refined and accurate for the purposes of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312df440",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6 = None # Your answer should be a ratio of integers\n",
    "# E.g., 1/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12c5f9",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://www.jcpcarchives.org/userfiles/values-of-p-Inference.jpg\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4c0fc",
   "metadata": {},
   "source": [
    "### Q7: Using the table above, state the strength of evidence against the *null hypothesis* for both the \"one-sided\" and \"two-sided\" p-values calcuated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a912d",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n",
    "- For the null hypothesis that... and a \"one-sided\" p-value of... simulated based on... we have XYZ evidence against...\n",
    "- For the null hypothesis that... and a \"two-sided\" p-value of... simulated based on... we have XYZ evidence against...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f76af9",
   "metadata": {},
   "source": [
    "### Q8: Explain the difference between $p$, $\\hat p$, and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208f2e6",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6041ad7",
   "metadata": {},
   "source": [
    "### Q9: change `reps` value in the code for `Q3/Q4` to `1000`, `10000`, and `100000` and compare the results to the following two figures\n",
    "\n",
    "#### Comment on the emerging simiarities to the two figures below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf6acf",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats; x=np.arange(0,80); prob=stats.binom(n=80, p=0.5).pmf(x)\n",
    "fig = px.bar(pd.DataFrame({'x':x/80,'probability':prob}), x='x', y='probability',\n",
    "             title='Theoretically Exact (Binomial) Sampling Distrubition of p-hat assuming \"H0: Random Guessing\"')\n",
    "fig.add_vline(x=observed_test_stat, line_dash=\"dash\", line_color=\"black\",\n",
    "              annotation_text=\"observed_test_stat (0.6125)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289dec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,200)\n",
    "observed_sample = np.array([0]*31+[1]*49)\n",
    "n = len(observed_sample)\n",
    "dens = stats.t(loc=0.5, df=len(observed_sample)-1, \n",
    "               scale=np.std(observed_sample, ddof=1)/n**0.5)\n",
    "# Another possible approximation could be based on `dens=stats.norm(loc=40, scale=0.5*80**0.5).pdf(x)`\n",
    "fig = px.line(pd.DataFrame({'x':x, 'density':dens.pdf(x)}), x='x', y='density',\n",
    "             title='A Continuous Approximation to the Theoretical (Binomial) Sampling Distribution (under H0) above')\n",
    "fig.add_vline(x=observed_test_stat, line_dash=\"dash\", line_color=\"black\",\n",
    "              annotation_text=\"observed_test_stat (0.6125)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e14c4",
   "metadata": {},
   "source": [
    "The Binomial sampling distrubition of $\\hat p$ given above is theoretically derived (and indeed exactly correct) based on only\\* assuming \n",
    "\n",
    "$$H_0: p=0.5 \\text{ (Random Guessing on $n=80$ attempts)}$$\n",
    "\n",
    "> \\**and, actually, that the guessed answers are independent of each other; so, answers don't change based on previous answers or affect future answers...*\n",
    "\n",
    "Thus, while this entails an assumption of the parameter $p=0.5$, there is actually no distributional assumptions made about the data itself (except the independence assumption, noted above). Because there are no distributional assumptions made about the data, a p-value calculated based on the theoretical binomial distribution is a (theoretical) \"non-parametric\" p-value (even despite assumption on the parameter $p=0.5$).  \n",
    "\n",
    "The continuous approximation to the theoretical Binomial sampling distribution (under H0) given above actually does entail an assumption about the distribution of the data; namely, that each invidual observation is an independent sample from *the same normally distributed population*. The \"less accurate\" this assumption is the worse the approximation will be.  Regardless, based on this assumption, the calculations used above are executed and produce the continuous approximation seen above.  And since there are indeed distributional assumptions made about the data in this case, a p-value calculated based on the continuous approximation of the theoretical binomial distribution is a (approximate, theoretical) \"parametric\" p-value.  Of course, if the assumption that each data point was an independent sample from the same normally distributed population were true, then this wouldn't be an approximate p-value at all: it would be a theoretical parameteric p-value. \n",
    "\n",
    "The theoretical nature of the two approaches distinguish them from the simulation-based p-value initially calculated using the dot plot. That p-value was a simulation (sampling) based approximation of the true (theoretical Binomial) sampling distribution, so it is a (simulated approximate) \"non-parametric\" p-value.  Thus, we've seen the following: the binomial distribution based p-value is a theoretically exact non-parametric p-value, for which we have a (theoretical) parametric and simulation based approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd39ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b01d76a",
   "metadata": {},
   "source": [
    "### Q10: Based on the p-values below, does the normality assumption of the t-test appear resonable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a6449",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c386236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretically Exact (Binomial) Sampling Distribution \"two sided\" p-value\n",
    "(1-stats.binom(n=80, p=0.5).cdf(49-1))*2\n",
    "# This calculates \"as or more extreme\" as the sum of all the probabilities (bin heights) that are located at \n",
    "# 49/80, 50/80, 51/80, ..., up to 80/80 (and then multiplies this sum by two since this distribution is symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a36393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the curve from 49/80 to 80/80 for the\n",
    "# continuous approximation to the binomial distirbution\n",
    "(1-dens.cdf(49/80))*2\n",
    "# This calculates \"as or more extreme as\" the area under the curve \n",
    "# from 49/80 to 80/80 (and multiply this sum by two since this distribution is symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is what the t-test computes on the basis of the \n",
    "# continuous approximation to the binomial distirbution\n",
    "stats.ttest_1samp(observed_sample, .5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd059dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# though we might alternatively consider the following \"continuity correction\" \n",
    "# that's more analagous to the way `49-1` is used for the theoretical p-value\n",
    "(1-dens.cdf((49-0.5)/80))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e593fe",
   "metadata": {},
   "source": [
    "### Q11: How would you make your simulated p-value more accurate to the true theoretical p-value given by the binomial distribution and what is the downside of doing so? \n",
    "\n",
    "> - Hint: The simulated sampling distribution examined in `Q3` and `Q4` is a simulation of the true theoretical binomial distribution in `Q8`.  This simulated sampling distribution would \"converge\" to the true theoretical binomial distribution if we kept running the simulation and adding simulated test statistics to the dot plot. The approximation in `Q10` is not quite right because it has continuous rather than discrete values; so, e.g., it could produce a proportion correct of 48.5/80 (which is not actually possible in the real experiment); but, the continuous approximation is still a pretty good approximation as judged by the similarity in the p-value calculation and the general shape of these two distributions (with the main difference of course just being that one is continuous and other other is descrete). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7b228",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n",
    "***Example Answer: using a simulated sampling distribution based on more simulated test statistics would be more similar and hence more accurate to the true theoretical binomial distribution; but, it would also take longer to run! There's also likely to be a point of dimenishing returns where smaller and smaller increases improvement require more and more simulated test statistics to the point where it doesn't really make sense to keep going.  There's probably some sort of tradeoff between the accuracy of the simulated p-value to the true theoretical binomial distribution and the amount of time it takes to produce the samples to achieve that accuracy.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cea6a",
   "metadata": {},
   "source": [
    "### Q12: Comment on the appropriateness of the normality assumption of the t-test again based on the figures below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861678b4",
   "metadata": {},
   "source": [
    "> Answer here...\n",
    "\n",
    "***Example answer: the normality assumption is clearly incorrect, as the 0/1 guessing distribution does not have the shape of a normal distribution and it's not even continuous as it is instead discrete; so, the t-test analysis does not seem like it would be appropriate. Nonetheless, the t-test p-value is not tremendously numerically different than the theoretically correct p-value of the binomial distribution analyses.  That said, without the 'continuity adjustment' correction the t-test is a bit smaller than the theoretically correct p-value, seemingly as a result of the increased \"power\" of the test due to its additional \"incorrect\" distributional assumptions, and the slightly smaller t-test p-value would have a different \"strength of evidence\" interpretation...  Pragmatically speaking, even though the normality assumption is not really correct, you can see that the overlay of the normal approximation on top of the 0/1 guessing distribution is kind of reasonable as a rough approximation; and, so, this is probably why the p-value from the t-test is in fact not so terribly far off from the theoretically correct p-value of the binomial distribution analyses (especially when the 'continuity correction' adjustment is made).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_sample = np.array([0]*31+[1]*49)\n",
    "observed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b868aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the assumption of the null hypothesis that random guessing $p=0.5$ is correct, then \n",
    "# then each of the 80 observed answers above is a sample from the following distribution\n",
    "x=[0,1]; prob=[0.5,0.5]\n",
    "px.bar(pd.DataFrame({'x':x,'probability':prob}), x='x', y='probability',\n",
    "       title=\"Distribution of Wrong/Right (0/1) Under 'H0: Random Guessing'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83949c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean of the \"single random guess\" distribution above is 0.5\n",
    "# Here's an overlayed normal approximation of this distribution \n",
    "import plotly.graph_objects as go\n",
    "fig = px.bar(pd.DataFrame({'x':x,'PMF/PDF':prob}), x='x', y='PMF/PDF',\n",
    "       title=\"Normal Distribution Approximation\")\n",
    "x_ = np.linspace(-1.5,1.5,200)\n",
    "fig.add_trace(go.Scatter(x=x_, y=stats.norm(0.5,0.5).pdf(x_), mode='lines', name='Normal Density'))\n",
    "# So rather than getting 0/1 (incorrect/correct) data points \n",
    "# a t-test is assumes the data points are numeric values sampled from the overlayed normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee75ac",
   "metadata": {},
   "source": [
    "The analyses in this assignment are based on constructing (through simuilation approximation, theoretical derivation, or continuous approximation) the sampling distribution of the sample proporition (average) under the assumption of the null hypothesis that $p=0.5$. The sampling distribution of a sample average (which a sample proportion is...) can be theoretically derived with the additional assumption that each data point is an independent sample from a normal population. Indeed, and this was how the continuous approximation to the Binomial sampling distribution was derived; and, this kind of analysis is called a t-test. The t-test is a parametric test since it requires an assumption of data normality; and, it's a theoretical test because once this assumption is made, the sampling distribution of the sample average is theoretically derived.\n",
    "\n",
    "The p-value for the random guessing binomial distribution is theoretically derived based only the on the assumption of \"random guessing\"; but, this is not a \"parametric\" analysis because the fact that it is uses a binomial distribution is not an assumption but rather that is derived only from the assumption of \"random guessing\". The statement of \"random guessing\" is enshrined in the assumption that the parameter $p=0.5$; however, this assumption alone is still viewed as \"non-parametric\" since it does not entail any specifications of the distribution of the data, which is what the term \"parametric\" refers to (even though it seems like it should be referring to parameters...).\n",
    "\n",
    "In summary, the p-value based on the binomial distribution is a theoretical non-parametric (truely correct) p-value; the original p-value from `Q3/Q4` is a simulated version of this non-parametric p-value; the t-test below is a theoretical parametric p-value (whose accuracy depends on the accuracy of the normality assumption entailed in the t-test)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
