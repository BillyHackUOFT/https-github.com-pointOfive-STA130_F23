{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ef9e6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA130 Tutorial 9: Classification (and Ethics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570a7d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This week we'll be doing something a little different, and we'll be focusing on ethical topics in anticipation of our upcoming embedded ethics guest lecture.\n",
    "Remember! It's always a good time for questions and discussions. If you don't understand something ask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f215dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# This Week's Vocab (10 minutes):\n",
    "If you are not familiar with any of these words, now is the time to ask!\n",
    "\n",
    "- Classification / Classifier\n",
    "- Prediction / Predictor(s)\n",
    "- Covariate(s)\n",
    "- Input(s) / Output(s)\n",
    "- Training set/sample\n",
    "- Validation\n",
    "- Testing set/sample (or test set)\n",
    "- Fitting a model\n",
    "- Confusion matrix\n",
    "- Category\n",
    "- Tree / Node\n",
    "- Terminal node (or leaf node)\n",
    "- True positive (sensitivity)\n",
    "- True negative (specificity)\n",
    "- False positive / False negative\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0e4ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ethics Primer (15 minutes)\n",
    "\n",
    "- When we do statistics in the real world, our actions have consequences.\n",
    "- Often classification models (and indeed other statistical methods we have learned such as linear regression and hypothesis test) are used to guide decisions.\n",
    "- Our actions can affect our employers or the public.\n",
    "- So when we make decisions, we must consider all the stakeholders who may be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c440e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example:\n",
    "Let's say we're building a classification model to decide whether to accept mortgage applications for a mortgage company.\n",
    "1. The first stakeholder is the mortgage company.\n",
    "    - We don't want to instruct them to accept bad mortgages which lose them money / cause instability.\n",
    "2. The second stakeholder are the applicants.\n",
    "    - We have a responsibility to ensure that people aren't being rejected for unfair reasons (e.g. race, gender, sexuality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431dced",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case Study and Discussion (30 minutes):\n",
    "\n",
    "Take ~10 minutes to skim this article:\n",
    "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
    "\n",
    "This article is an interesting read, and is worthwhile to finish it on your own time after tutorial. But here is the spark-notes version:\n",
    "\n",
    "- A company, NorthPointe, made and sold a tool called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) to help judges, probation officers, and parole offices to assess the likelihood of a criminal to re-offend.\n",
    "    - This data was used to determine criminal sentences, decide whether prisoners should be eligible for early release etc.\n",
    "- COMPAS was fundamentally a classification model, it classified defendants into risk classes of likelihood to reoffend (commit more crimes after being released) based on some input data.\n",
    "\n",
    "- Among defendants who did not reoffend, COMPAS predicted 45% of black defendants and 23% of white defendants were at higher risk to reoffend.\n",
    "- Among defendants who did reoffend, COMPAS predicted 28% of black defendants and 48% of white defendants were at lower risk to reoffend.\n",
    "- Controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 more likely to be assigned higher risks of recidivism by COMPAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fb15c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This case touches on the idea of *algorithmic bias,* systematic and repeatable errors that create unfair outcomes.\n",
    "\n",
    "Discuss the following questions for a few minutes in small groups, then share answers with the class:\n",
    "- List as many relevant stakeholders in this situation as you can.\n",
    "- COMPAS is similarly *accurate* on both racial groups. Why does this not correlate to fairness?\n",
    "- Why might COMPAS have generated these outcomes? (e.g. What biases might be in the data it built its model from? Might they have fit a poor model?)\n",
    "    - What could the model-designer have potentially done to mitigate these problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1a0d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Confusion Matrices (15 minutes)\n",
    "Let's recall how to read a simple confusion Matrix. Suppose that we have a classification model designed to judge whether patients have a certain disease, called disease D.\n",
    "On a testing dataset, it has the following confusion matrix (columns are predicted labels, rows are real labels)\n",
    "\n",
    "|            | P  | N   |\n",
    "|-------------|----|-----|\n",
    "| P | 17 | 2   |\n",
    "| N           | 17 | 168 |\n",
    "\n",
    "- What do the numbers in the different cells represent?\n",
    "- What are the metrics (accuracy, sensitivity,  specificity)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec6d27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But which metrics do we care about?\n",
    "- Inaccuracy is obviously bad.\n",
    "- A false negative could potentially be very bad for a patient.\n",
    "- Excessive false positives could drain resources (such as medicines, hospital beds, doctor time etc.) from the system.\n",
    "- A false positive might cause a patient to undergo unnecessary (potentially dangerous) medical interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3787570",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion (30 minutes)\n",
    "These types of considerations are situation dependant. Thus let's practice thinking about them (i.e. identify stakeholders, consequences, etc.), reason about which metrics matter the most, and what trade-offs we can accept.\n",
    "\n",
    "- The above case about medical testing where the disease D being tested for is the common cold.\n",
    "- The above case about medical testing where the disease D being tested for is a sexually transmitted infection like HPV or HIV.\n",
    "- The above case about medical testing where the disease D being tested for is a serious life-threatening condition like cancer.\n",
    "- The NorthPointe COMPAS case.\n",
    "    - What extra considerations should we be making in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397e626",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial Assignment (get started... )\n",
    "Suppose we are designing a classification model for reviewing applications to a prestigious medical school. Discuss how much we should value the different metrics for judging performance of our model. (Hint: in doing so you should identify stakeholders and what effects false positives and negatives would have on them?)\n",
    "\n",
    "### Notes on approaching the writing prompt\n",
    "\n",
    "- Hand in the assignment on Quercus\n",
    "- Use full sentences\n",
    "- Grammar is *not* the main focus of the assessment, but it is important that you communicate in a clear and professional manner (without slang or emojis) \n",
    "- Aim for 200 - 500 words\n",
    "- Do not spend more than 90 minutes on the prompt (unless you really need to...)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
